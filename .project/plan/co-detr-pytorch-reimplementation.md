# Project Execution Plan: Co-DETR Pure PyTorch Reimplementation
*Generated by StrategicPlannerAI*

---

## 1. Objective
Reimplement Co-Deformable DETR (ResNet-50 backbone) from MMDetection/MMCV to pure PyTorch with minimal external dependencies, enabling training, fine-tuning, and inference on custom YOLOv5-format datasets with Kaggle/Colab compatibility.

## 2. Scope
### In-Scope
- **Core Architecture Implementation**:
  - Co-Deformable DETR detector with ResNet-50 backbone
  - Deformable attention mechanism (transformer encoder/decoder)
  - Query denoising (CDN - Collaborative Denoising)
  - Multi-scale feature extraction (FPN-like neck)
  - Detection heads (classification + bounding box regression)
  - Auxiliary heads for collaborative training (RPN, RoI Head, ATSS)

- **Data Pipeline**:
  - YOLOv5 format dataset loader (txt annotations with class_id, cx, cy, w, h)
  - Data augmentation (RandomFlip, Resize, Normalize, Mosaic/MixUp optional)
  - Batch collation and padding

- **Training Infrastructure**:
  - Multi-GPU training support (DistributedDataParallel)
  - Mixed precision training (AMP)
  - Gradient checkpointing for memory efficiency
  - Loss functions: Focal Loss, L1 Loss, GIoU Loss
  - Hungarian matching for DETR
  - High-level configuration system (YAML-based)

- **Inference & Evaluation**:
  - Single/batch image inference
  - Post-processing (NMS, score filtering)
  - Custom evaluation metrics (without pycocotools)
  - Model checkpoint saving/loading

- **Weight Conversion Tool**:
  - Script to convert MMDetection pretrained weights to new format
  - Mapping between old and new model structures

- **High-Level API**:
  - Easy model instantiation: `model = CoDeformableDETR.from_config(config_path)`
  - Simple training interface: `trainer.train(model, train_loader, val_loader)`
  - Simple inference interface: `results = model.predict(image_path)`

- **Documentation & Examples**:
  - Training script for YOLOv5 format datasets
  - Inference script with visualization
  - README with setup and usage instructions
  - Configuration examples

### Out-of-Scope
- Co-DINO variant (focus on Co-Deformable DETR only)
- Other backbones (Swin Transformer, ViT) - only ResNet-50
- Instance segmentation (mask head)
- COCO/LVIS specific optimizations
- TorchScript/ONNX export (can be added later)
- Advanced data augmentation (CopyPaste, LSJ) - basic augmentations only
- Web UI or deployment tools

## 3. Assumptions & Dependencies
### Assumptions
- Users have CUDA-capable GPUs available (for training)
- Input images are in standard formats (jpg, png)
- YOLOv5 format follows standard convention: `class_id cx cy w h` (normalized 0-1)
- Dataset structure: `data/images/` and `data/labels/` folders with matching filenames
- Python >= 3.8 environment
- User will train on relatively modern datasets (not legacy formats)

### Dependencies (Minimal Set)
- **Core**:
  - `torch >= 2.0.0` (with CUDA support)
  - `torchvision >= 0.15.0` (for ResNet backbone, transforms, NMS)
  - `numpy` (for numerical operations)
  - `Pillow` or `opencv-python` (for image I/O)
  
- **Configuration & Utilities**:
  - `PyYAML` (for config files)
  - `tqdm` (for progress bars)
  - `tensorboard` (for training visualization - optional)

- **NOT using**:
  - ~~mmcv, mmdet, mmengine~~ (outdated)
  - ~~pycocotools~~ (as requested)
  - ~~scipy, scikit-learn~~ (unnecessary)

## 4. Execution Steps

### Phase 1: Project Setup & Core Architecture (Foundation)

- [ ] **Step 1.1: Initialize Project Structure**
  - Create clean project directory structure:
    ```
    Co-DETR-pytorch/
    ├── codetr/
    │   ├── __init__.py
    │   ├── models/
    │   │   ├── __init__.py
    │   │   ├── backbone/
    │   │   │   ├── __init__.py
    │   │   │   └── resnet.py (wrapper for torchvision ResNet)
    │   │   ├── neck/
    │   │   │   ├── __init__.py
    │   │   │   └── channel_mapper.py (FPN-like feature pyramid)
    │   │   ├── transformer/
    │   │   │   ├── __init__.py
    │   │   │   ├── attention.py (multi-scale deformable attention)
    │   │   │   ├── encoder.py
    │   │   │   ├── decoder.py
    │   │   │   └── transformer.py (main transformer module)
    │   │   ├── heads/
    │   │   │   ├── __init__.py
    │   │   │   ├── detr_head.py (main Co-Deformable DETR head)
    │   │   │   ├── rpn_head.py (auxiliary RPN)
    │   │   │   ├── roi_head.py (auxiliary RoI)
    │   │   │   └── atss_head.py (auxiliary ATSS)
    │   │   ├── losses/
    │   │   │   ├── __init__.py
    │   │   │   ├── focal_loss.py
    │   │   │   ├── giou_loss.py
    │   │   │   └── l1_loss.py
    │   │   ├── matchers/
    │   │   │   ├── __init__.py
    │   │   │   └── hungarian_matcher.py
    │   │   ├── utils/
    │   │   │   ├── __init__.py
    │   │   │   ├── box_ops.py (bbox utilities)
    │   │   │   ├── misc.py (various utilities)
    │   │   │   └── position_encoding.py (sine positional encoding)
    │   │   └── detector.py (main Co-DETR detector class)
    │   ├── data/
    │   │   ├── __init__.py
    │   │   ├── datasets/
    │   │   │   ├── __init__.py
    │   │   │   └── yolo_dataset.py
    │   │   ├── transforms/
    │   │   │   ├── __init__.py
    │   │   │   └── transforms.py
    │   │   └── dataloader.py
    │   ├── engine/
    │   │   ├── __init__.py
    │   │   ├── trainer.py
    │   │   ├── evaluator.py
    │   │   └── hooks.py (checkpoint, logging, etc.)
    │   ├── configs/
    │   │   ├── __init__.py
    │   │   ├── config.py (config parser)
    │   │   └── defaults/
    │   │       └── co_deformable_detr_r50.yaml
    │   └── utils/
    │       ├── __init__.py
    │       ├── checkpoint.py
    │       ├── distributed.py
    │       └── logger.py
    ├── tools/
    │   ├── train.py
    │   ├── inference.py
    │   ├── convert_weights.py (MMDet -> PyTorch)
    │   └── visualize.py
    ├── configs/
    │   └── co_deformable_detr_r50_yolo.yaml
    ├── requirements.txt
    ├── setup.py
    └── README.md
    ```
  - Create `requirements.txt` with minimal dependencies
  - Create basic `setup.py` for package installation

- [ ] **Step 1.2: Implement Backbone (ResNet-50)**
  - Create `codetr/models/backbone/resnet.py`
  - Wrapper around `torchvision.models.resnet50`
  - Extract multi-scale features from stages C3, C4, C5 (as in original config)
  - Support frozen_stages (freeze BN in stage 1)
  - Support pretrained weight loading from torchvision

- [ ] **Step 1.3: Implement Neck (Channel Mapper)**
  - Create `codetr/models/neck/channel_mapper.py`
  - 1x1 convolution to project [512, 1024, 2048] -> [256, 256, 256]
  - Add one extra level via 3x3 conv stride-2 on C5: [256, 256, 256, 256]
  - GroupNorm for normalization (GN-32)

- [ ] **Step 1.4: Implement Utility Functions**
  - `codetr/models/utils/box_ops.py`:
    - `bbox_xyxy_to_cxcywh`, `bbox_cxcywh_to_xyxy`
    - `box_iou`, `generalized_box_iou` (for GIoU loss)
    - `inverse_sigmoid` helper
  - `codetr/models/utils/position_encoding.py`:
    - Sine positional encoding (2D)
  - `codetr/models/utils/misc.py`:
    - Nested tensor utilities
    - `get_valid_ratio` for attention masks

### Phase 2: Transformer Components (Core Innovation)

- [ ] **Step 2.1: Implement Multi-Scale Deformable Attention**
  - Create `codetr/models/transformer/attention.py`
  - **Critical Component**: This is the core of deformable attention
  - Implement `MultiScaleDeformableAttention` module:
    - Sampling offsets prediction
    - Attention weights prediction
    - Multi-scale feature sampling via grid_sample or custom CUDA op
  - **Note**: For initial version, use PyTorch's `grid_sample` (slower but no custom CUDA)
  - For production: can add optional custom CUDA kernel later

- [ ] **Step 2.2: Implement Transformer Encoder**
  - Create `codetr/models/transformer/encoder.py`
  - 6-layer encoder with multi-scale deformable self-attention
  - FFN with dropout
  - Layer normalization
  - Residual connections

- [ ] **Step 2.3: Implement Transformer Decoder**
  - Create `codetr/models/transformer/decoder.py`
  - Implement `CoDeformableDetrTransformerDecoder`:
    - 6-layer decoder
    - Self-attention on queries
    - Multi-scale deformable cross-attention (query -> encoder features)
    - FFN with dropout
    - Support iterative bounding box refinement
    - Return intermediate layer outputs

- [ ] **Step 2.4: Implement Main Transformer Module**
  - Create `codetr/models/transformer/transformer.py`
  - Implement `CoDeformableDetrTransformer`:
    - Integrate encoder + decoder
    - Generate reference points from encoder features (two-stage)
    - Position encoding integration
    - Support for auxiliary decoder (for collaborative heads)
    - `forward()` and `forward_aux()` methods

### Phase 3: Loss Functions & Matching

- [ ] **Step 3.1: Implement Loss Functions**
  - `codetr/models/losses/focal_loss.py`: Focal Loss for classification
  - `codetr/models/losses/l1_loss.py`: L1 Loss for bbox regression
  - `codetr/models/losses/giou_loss.py`: Generalized IoU Loss

- [ ] **Step 3.2: Implement Hungarian Matcher**
  - Create `codetr/models/matchers/hungarian_matcher.py`
  - Bipartite matching between predictions and ground truth
  - Cost matrix: weighted sum of classification cost, L1 cost, GIoU cost
  - Use `scipy.optimize.linear_sum_assignment` OR implement pure PyTorch version
  - **Decision**: Implement pure PyTorch version to avoid scipy dependency

### Phase 4: Detection Heads

- [ ] **Step 4.1: Implement Main DETR Head**
  - Create `codetr/models/heads/detr_head.py`
  - Implement `CoDeformDETRHead`:
    - Classification branch (Linear)
    - Regression branch (MLP: Linear -> ReLU -> Linear)
    - 6 prediction layers (one per decoder layer) with/without weight sharing
    - Query embedding (300 learnable queries)
    - Loss computation using Hungarian matching
    - Post-processing: top-k selection, score thresholding, NMS
    - Support for auxiliary query heads (for collaborative training)

- [ ] **Step 4.2: Implement Auxiliary RPN Head**
  - Create `codetr/models/heads/rpn_head.py`
  - Standard RPN with anchor generation
  - Classification + regression on anchors
  - Proposal generation via NMS

- [ ] **Step 4.3: Implement Auxiliary RoI Head**
  - Create `codetr/models/heads/roi_head.py`
  - RoI Align pooling from encoder features
  - Two FC layers + classification/regression
  - Sampling strategy (positive/negative)

- [ ] **Step 4.4: Implement Auxiliary ATSS Head**
  - Create `codetr/models/heads/atss_head.py`
  - Adaptive Training Sample Selection
  - Anchor-based detection with centerness
  - Similar structure to RetinaNet but with ATSS assignment

### Phase 5: Query Denoising (CDN)

- [ ] **Step 5.1: Implement Query Denoising Generator**
  - Create `codetr/models/utils/query_denoising.py`
  - Implement `CdnQueryGenerator`:
    - Add noisy ground truth queries to training
    - Label noise: randomly flip class labels
    - Box noise: add Gaussian noise to bbox coordinates
    - Group-based denoising (multiple noise groups)
    - Generate attention masks to prevent query leakage
  - This stabilizes training and improves performance

### Phase 6: Main Detector Assembly

- [ ] **Step 6.1: Implement Main Co-DETR Detector**
  - Create `codetr/models/detector.py`
  - Implement `CoDETR` class:
    - Integrate backbone, neck, transformer, all heads
    - Multi-head architecture: query_head (DETR) + rpn_head + roi_head + atss_head
    - Forward pass for training: compute all losses with proper weighting
    - Forward pass for inference: use best performing head (configurable)
    - Collaborative training: auxiliary heads provide positive coordinates to query head
    - Lambda weighting for loss balancing

- [ ] **Step 6.2: Implement High-Level API**
  - Add factory methods:
    - `CoDETR.from_config(config_path)` - load from YAML config
    - `CoDETR.from_pretrained(checkpoint_path)` - load pretrained model
  - Simplified inference:
    - `model.predict(image_path)` - single image inference with post-processing
    - `model.predict_batch(image_list)` - batch inference
  - Return format: list of dicts with 'boxes', 'scores', 'labels'

### Phase 7: Data Pipeline

- [ ] **Step 7.1: Implement YOLOv5 Dataset Loader**
  - Create `codetr/data/datasets/yolo_dataset.py`
  - Implement `YOLODataset`:
    - Parse YOLOv5 format: `images/` and `labels/` folders
    - Read `.txt` annotations: `class_id cx cy w h` (normalized)
    - Convert to absolute coordinates
    - Handle train/val split via text file list or folder structure
    - Return: image tensor, bbox tensor, label tensor, image metadata

- [ ] **Step 7.2: Implement Data Transforms**
  - Create `codetr/data/transforms/transforms.py`
  - Implement common transforms:
    - `Resize` (keep aspect ratio)
    - `RandomFlip` (horizontal)
    - `Normalize` (ImageNet mean/std)
    - `Pad` (to divisible size)
    - `ToTensor`
  - Compose transforms into pipeline
  - Handle bbox coordinate transformations

- [ ] **Step 7.3: Implement DataLoader & Collation**
  - Create `codetr/data/dataloader.py`
  - Custom collate function for batching (images have different sizes)
  - Handle padding to max size in batch
  - Support distributed sampling (DistributedSampler)

### Phase 8: Training Infrastructure

- [ ] **Step 8.1: Implement Configuration System**
  - Create `codetr/configs/config.py`
  - YAML-based configuration parser
  - Support for nested configs and inheritance
  - Default config: `codetr/configs/defaults/co_deformable_detr_r50.yaml`
  - Parameters: model architecture, training hyperparameters, data paths, etc.

- [ ] **Step 8.2: Implement Training Engine**
  - Create `codetr/engine/trainer.py`
  - Implement `Trainer` class:
    - Training loop with epoch/iteration management
    - Loss computation and backpropagation
    - Gradient clipping (max_norm=0.1)
    - Learning rate scheduling (step decay)
    - Distributed training support (DDP)
    - Mixed precision training (autocast + GradScaler)
    - Checkpoint saving at intervals
    - Logging (console + TensorBoard)
    - Validation loop

- [ ] **Step 8.3: Implement Distributed Training Support**
  - Create `codetr/utils/distributed.py`
  - Helper functions:
    - `init_distributed_mode()` - setup DDP
    - `get_rank()`, `get_world_size()`
    - `reduce_dict()` - aggregate metrics across GPUs
    - `is_main_process()` - for logging/saving on rank 0 only

- [ ] **Step 8.4: Implement Gradient Checkpointing**
  - Add gradient checkpointing option in transformer encoder
  - Use `torch.utils.checkpoint.checkpoint` for memory-intensive layers
  - Configurable: enable/disable via config

### Phase 9: Evaluation & Inference

- [ ] **Step 9.1: Implement Custom Evaluation Metrics**
  - Create `codetr/engine/evaluator.py`
  - Implement custom mAP calculation (without pycocotools):
    - Compute precision-recall curves
    - Calculate AP for each class at IoU thresholds [0.5:0.95:0.05]
    - Average over classes and IoU thresholds
  - Other metrics: AP@0.5, AP@0.75, precision, recall

- [ ] **Step 9.2: Implement Inference Pipeline**
  - Create `tools/inference.py`
  - Single image inference:
    - Load image
    - Preprocess (resize, normalize, pad)
    - Forward pass
    - Post-process (NMS, score filtering)
    - Return detections
  - Batch inference support
  - Visualization: draw bboxes on image

- [ ] **Step 9.3: Implement Visualization Tools**
  - Create `tools/visualize.py`
  - Functions to:
    - Draw bounding boxes with labels and scores
    - Save visualized images
    - Create video from image sequence (optional)

### Phase 10: Weight Conversion Tool

- [ ] **Step 10.1: Analyze MMDetection Weight Structure**
  - Load MMDetection checkpoint
  - Inspect state_dict keys and shapes
  - Map old keys to new model structure
  - Handle differences in naming conventions

- [ ] **Step 10.2: Implement Conversion Script**
  - Create `tools/convert_weights.py`
  - Load MMDetection checkpoint (`.pth` file)
  - Create mapping dictionary: old_key -> new_key
  - Handle shape mismatches (if any)
  - Convert and save to new format
  - Verification: load converted weights and test forward pass

- [ ] **Step 10.3: Test Converted Weights**
  - Load converted weights into new model
  - Run inference on sample images
  - Compare outputs with original MMDetection model (qualitatively)
  - Document conversion process and limitations

### Phase 11: Training & Inference Scripts

- [ ] **Step 11.1: Create Training Script**
  - Create `tools/train.py`
  - Command-line interface:
    - `python tools/train.py --config configs/co_deformable_detr_r50_yolo.yaml --data_root data/`
  - Options: resume training, multi-GPU, mixed precision, etc.
  - Load config, create model, dataset, trainer
  - Start training loop
  - Save final checkpoint

- [ ] **Step 11.2: Create Inference Script**
  - Create `tools/inference.py`
  - Command-line interface:
    - `python tools/inference.py --checkpoint checkpoints/best.pth --image test.jpg --output output/`
  - Load model from checkpoint
  - Run inference
  - Save/display results

- [ ] **Step 11.3: Create Example Config for YOLOv5 Dataset**
  - Create `configs/co_deformable_detr_r50_yolo.yaml`
  - Configure for YOLOv5 format dataset
  - Set training hyperparameters (lr, batch size, epochs, etc.)
  - Set data paths and augmentation

### Phase 12: Testing & Validation

- [ ] **Step 12.1: Unit Tests for Core Components**
  - Test backbone: verify output shapes
  - Test transformer: verify attention computation
  - Test heads: verify loss computation
  - Test data loader: verify batch shapes and values
  - Test box operations: verify coordinate conversions

- [ ] **Step 12.2: Integration Test: Overfitting Small Dataset**
  - Create tiny dataset (10-20 images)
  - Train model to overfit (should reach near 100% AP)
  - This validates that the entire pipeline works correctly
  - Debug any issues in loss computation, gradient flow, etc.

- [ ] **Step 12.3: Integration Test: Training on Public Dataset**
  - Convert a public dataset (e.g., VOC, custom dataset) to YOLOv5 format
  - Train for a few epochs
  - Validate that loss decreases and metrics improve
  - Compare performance with baseline (if available)

### Phase 13: Documentation & Polishing

- [ ] **Step 13.1: Write Comprehensive README**
  - Project overview and features
  - Installation instructions
  - Quick start guide
  - Training guide (with YOLOv5 dataset)
  - Inference guide
  - Configuration guide
  - FAQ and troubleshooting

- [ ] **Step 13.2: Write Detailed Documentation**
  - Model architecture explanation
  - Configuration file reference
  - API documentation (docstrings)
  - Training tips and best practices
  - Known issues and limitations

- [ ] **Step 13.3: Create Example Notebooks**
  - Jupyter notebook for Colab: training tutorial
  - Jupyter notebook for Kaggle: training tutorial
  - Jupyter notebook for inference and visualization

- [ ] **Step 13.4: Code Quality & Cleanup**
  - Add type hints to all functions
  - Add comprehensive docstrings
  - Format code with `black` or `autopep8`
  - Remove debug print statements
  - Optimize performance bottlenecks (if identified)

### Phase 14: Optional Enhancements (Nice-to-Have)

- [ ] **Step 14.1: Advanced Data Augmentation**
  - Implement Mosaic augmentation (combine 4 images)
  - Implement MixUp augmentation
  - Configurable augmentation pipeline

- [ ] **Step 14.2: Model Export**
  - ONNX export support
  - TorchScript export support
  - Quantization support (INT8)

- [ ] **Step 14.3: Multi-Dataset Support**
  - COCO format dataset loader
  - Pascal VOC format dataset loader
  - Unified dataset interface

- [ ] **Step 14.4: Experiment Tracking Integration**
  - Weights & Biases integration
  - MLflow integration
  - Neptune.ai integration

## 5. Verification & Success Criteria

### Functional Success Criteria
- [ ] Model can be instantiated from config file without errors
- [ ] Model can perform forward pass on dummy input
- [ ] Training loop runs for at least 10 epochs without crashing
- [ ] Loss values decrease over training epochs
- [ ] Model can load and save checkpoints correctly
- [ ] Inference script produces valid detection results
- [ ] Converted MMDetection weights load successfully
- [ ] Multi-GPU training works correctly (tested with 2+ GPUs)
- [ ] Mixed precision training reduces memory usage without accuracy loss

### Performance Success Criteria
- [ ] Training on tiny dataset: achieves >95% AP (overfitting test)
- [ ] Training on custom YOLOv5 dataset: achieves reasonable AP (>0.3 on moderately difficult dataset)
- [ ] Inference speed: <100ms per image on GPU (ResNet-50, 800x800 input)
- [ ] Memory usage: Training with batch_size=2 fits in 11GB GPU (mixed precision)
- [ ] If MMDet weights converted: similar qualitative results on test images

### Code Quality Criteria
- [ ] All core modules have docstrings
- [ ] No hardcoded paths or magic numbers in core code
- [ ] Configuration-driven design (no need to modify code for common use cases)
- [ ] Code passes basic linting (no obvious errors)
- [ ] README is clear and complete

### User Experience Criteria
- [ ] User can train model with single command: `python tools/train.py --config ...`
- [ ] User can run inference with single command: `python tools/inference.py --checkpoint ... --image ...`
- [ ] Setup time: <10 minutes from clone to first training run (with dependencies installed)
- [ ] Error messages are informative (e.g., missing config keys, shape mismatches)

### Compatibility Criteria
- [ ] Code runs on Google Colab without modification
- [ ] Code runs on Kaggle Notebooks without modification
- [ ] Works with PyTorch 2.0+ and CUDA 11.8+
- [ ] Works on Windows, Linux, macOS (for inference at least)

---

## 6. Risk Mitigation

### Risk 1: Multi-Scale Deformable Attention Performance
- **Risk**: Pure PyTorch implementation may be too slow
- **Mitigation**: 
  - Start with PyTorch `grid_sample` for correctness
  - Profile and optimize critical paths
  - If needed, add optional custom CUDA kernel (out of initial scope)
  - Provide clear documentation on performance trade-offs

### Risk 2: Weight Conversion Failures
- **Risk**: MMDetection weights may not map cleanly to new structure
- **Mitigation**:
  - Thorough analysis of weight structures before conversion
  - Implement robust key mapping with fallbacks
  - If conversion fails, document and provide training-from-scratch alternative
  - Success criterion: conversion works OR clear documentation on training from scratch

### Risk 3: Complex Collaborative Training Logic
- **Risk**: Multi-head training with auxiliary losses is complex and error-prone
- **Mitigation**:
  - Implement and test each head independently first
  - Add extensive logging to track each loss component
  - Test with simplified config (e.g., DETR head only) before full Co-DETR
  - Use overfitting test to validate gradient flow

### Risk 4: Memory Issues on Limited Hardware
- **Risk**: Users may not have sufficient GPU memory
- **Mitigation**:
  - Provide multiple config variants (batch_size 1, 2, 4)
  - Document memory requirements clearly
  - Implement gradient checkpointing (reduces memory)
  - Support mixed precision training (reduces memory)
  - Provide tips for reducing model size (fewer queries, smaller backbone)

### Risk 5: Dependency Compatibility Issues
- **Risk**: PyTorch/CUDA version mismatches on Colab/Kaggle
- **Mitigation**:
  - Test on fresh Colab/Kaggle environments
  - Pin dependency versions in requirements.txt
  - Provide troubleshooting guide for common issues
  - Use widely compatible PyTorch versions (2.0+)

---

## 7. Timeline Estimate (for reference)
- **Phase 1-2 (Foundation + Transformer)**: ~1-2 weeks
- **Phase 3-6 (Losses, Heads, Detector)**: ~1-2 weeks  
- **Phase 7-9 (Data, Training, Evaluation)**: ~1 week
- **Phase 10 (Weight Conversion)**: ~2-3 days
- **Phase 11-12 (Scripts, Testing)**: ~3-4 days
- **Phase 13 (Documentation)**: ~2-3 days
- **Total Estimate**: ~4-6 weeks for complete implementation

---

## 8. Notes & Recommendations

### Development Strategy
1. **Start Simple, Add Complexity Gradually**:
   - First implement basic DETR without auxiliary heads
   - Validate with overfitting test
   - Then add auxiliary heads one by one

2. **Test Early and Often**:
   - Unit test each component as you build it
   - Run integration tests frequently
   - Keep a tiny dataset for quick validation

3. **Reference Original Implementation**:
   - Keep MMDetection code as reference
   - Verify shapes and logic match original
   - Don't blindly copy - understand and adapt

4. **Modular Design**:
   - Each component should be independently testable
   - Use clear interfaces between modules
   - Makes debugging much easier

### Implementation Priorities
1. **Must-Have First** (Phases 1-9, 11): Core functionality
2. **High Value Second** (Phase 10, 12-13): Weight conversion, testing, docs
3. **Nice-to-Have Last** (Phase 14): Enhancements

### Key Technical Decisions
- **Deformable Attention**: Start with PyTorch implementation (no custom CUDA initially)
- **Hungarian Matching**: Pure PyTorch implementation (no scipy)
- **Config System**: YAML-based (simple and readable)
- **Logging**: TensorBoard (widely supported, no extra account needed)

---

**Next Steps**: 
1. Review this plan and confirm alignment with your requirements
2. Begin Phase 1: Project Setup & Core Architecture
3. Maintain a development log to track progress and issues

**Contact**: For questions or clarifications during implementation, refer back to this plan and update as needed.
